# Kubernetes Deployment for EpisodicMemory
# Production-ready deployment with health probes
#
# Features:
# - Liveness probe: Restarts pod if service is completely dead
# - Readiness probe: Removes pod from service if dependencies unhealthy
# - Resource limits and requests
# - Rolling update strategy
# - Pod disruption budget
# - Horizontal pod autoscaling

apiVersion: apps/v1
kind: Deployment
metadata:
  name: episodic-memory
  namespace: default
  labels:
    app: episodic-memory
    version: v0.1.0
    tier: backend
spec:
  replicas: 3  # Start with 3 replicas for high availability
  revisionHistoryLimit: 10

  # Rolling update strategy (zero-downtime deployments)
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1  # Allow 1 extra pod during update
      maxUnavailable: 0  # Never take down all pods at once

  selector:
    matchLabels:
      app: episodic-memory

  template:
    metadata:
      labels:
        app: episodic-memory
        version: v0.1.0
      annotations:
        # Prometheus scraping annotations
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"

    spec:
      # Service account for RBAC
      serviceAccountName: episodic-memory

      # Pod anti-affinity (spread pods across nodes)
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - episodic-memory
                topologyKey: kubernetes.io/hostname

      # Init containers (Phase 3: Database migrations)
      # initContainers:
      #   - name: db-migrate
      #     image: episodic-memory:latest
      #     command: ["python3", "-m", "alembic", "upgrade", "head"]

      containers:
        - name: api-server
          image: episodic-memory:latest  # Replace with actual registry
          imagePullPolicy: IfNotPresent

          # Container ports
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP

          # Environment variables (from ConfigMap and Secrets)
          envFrom:
            - configMapRef:
                name: episodic-memory-config
            - secretRef:
                name: episodic-memory-secrets

          # Additional environment variables
          env:
            - name: LOGGING_JSON_OUTPUT
              value: "true"  # JSON logs for Loki/ELK
            - name: LOGGING_LEVEL
              value: "INFO"
            - name: LOGGING_ENVIRONMENT
              value: "production"

          # Resource limits and requests
          resources:
            requests:
              cpu: 500m  # 0.5 CPU core
              memory: 1Gi  # 1 GB RAM
            limits:
              cpu: 2000m  # 2 CPU cores (burst)
              memory: 4Gi  # 4 GB RAM (max)

          # Liveness probe: Is the pod alive?
          # If this fails, Kubernetes will restart the pod
          livenessProbe:
            httpGet:
              path: /health/liveness
              port: http
              scheme: HTTP
            initialDelaySeconds: 30  # Wait for service to start
            periodSeconds: 10  # Check every 10 seconds
            timeoutSeconds: 5  # Timeout after 5 seconds
            successThreshold: 1  # 1 success = healthy
            failureThreshold: 3  # 3 consecutive failures = restart pod

          # Readiness probe: Is the pod ready to accept traffic?
          # If this fails, pod is removed from service endpoints
          readinessProbe:
            httpGet:
              path: /health/readiness
              port: http
              scheme: HTTP
            initialDelaySeconds: 10  # Check readiness early
            periodSeconds: 5  # Check every 5 seconds
            timeoutSeconds: 3  # Faster timeout for readiness
            successThreshold: 1  # 1 success = ready
            failureThreshold: 2  # 2 consecutive failures = not ready

          # Startup probe: Is the service starting up?
          # Protects slow-starting services from being killed by liveness probe
          startupProbe:
            httpGet:
              path: /health/liveness
              port: http
              scheme: HTTP
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 12  # Allow 60 seconds for startup (12 * 5s)

          # Volume mounts
          volumeMounts:
            - name: data
              mountPath: /app/data
            - name: logs
              mountPath: /app/logs

          # Security context
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            readOnlyRootFilesystem: false  # Set to true after Phase 3
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL

      # Volumes
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: episodic-memory-data
        - name: logs
          emptyDir: {}  # Ephemeral logs (sent to Loki)

      # Termination grace period (allow graceful shutdown)
      terminationGracePeriodSeconds: 30

---
# Service (load balancer)
apiVersion: v1
kind: Service
metadata:
  name: episodic-memory-svc
  namespace: default
  labels:
    app: episodic-memory
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 80
      targetPort: http
      protocol: TCP
  selector:
    app: episodic-memory

---
# Pod Disruption Budget (ensure availability during updates)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: episodic-memory-pdb
  namespace: default
spec:
  minAvailable: 1  # Always keep at least 1 pod running
  selector:
    matchLabels:
      app: episodic-memory

---
# Horizontal Pod Autoscaler (auto-scaling based on CPU/memory)
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: episodic-memory-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: episodic-memory

  minReplicas: 3  # Minimum replicas for high availability
  maxReplicas: 10  # Maximum replicas for peak load

  metrics:
    # Scale based on CPU utilization
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70  # Scale up when CPU > 70%

    # Scale based on memory utilization
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80  # Scale up when memory > 80%

  # Scaling behavior (Phase 3: Fine-tune based on load patterns)
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60  # Wait 60s before scaling up
      policies:
        - type: Percent
          value: 50  # Scale up by 50% at a time
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 minutes before scaling down
      policies:
        - type: Pods
          value: 1  # Scale down 1 pod at a time
          periodSeconds: 60
